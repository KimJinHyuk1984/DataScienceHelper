# pages/2_ğŸ› ï¸_ë°ì´í„°_ì „ì²˜ë¦¬.py
import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder
# from utils_ml import get_dataset # ì´ í˜ì´ì§€ì—ì„œëŠ” ì•„ë˜ ìì²´ ë°ì´í„° ìƒì„± í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
                                 # í•„ìš”ì— ë”°ë¼ utils_mlì˜ ë‹¤ë¥¸ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤ë©´ ì£¼ì„ í•´ì œ ê°€ëŠ¥.

st.header("2. ë°ì´í„° ì „ì²˜ë¦¬ (Data Preprocessing)")
st.markdown("""
ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ì…ë ¥ ë°ì´í„°ì˜ í’ˆì§ˆì— í¬ê²Œ ì¢Œìš°ë©ë‹ˆë‹¤. ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ì›ì‹œ(raw) ë°ì´í„°ë¥¼ ëª¨ë¸ í•™ìŠµì— ì í•©í•œ í˜•íƒœë¡œ ê°€ê³µí•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
ì£¼ìš” ì „ì²˜ë¦¬ ì‘ì—…ìœ¼ë¡œëŠ” ê²°ì¸¡ì¹˜ ì²˜ë¦¬, íŠ¹ì„± ìŠ¤ì¼€ì¼ë§, ë²”ì£¼í˜• ë°ì´í„° ì¸ì½”ë”©, ë°ì´í„° ë¶„í•  ë“±ì´ ìˆìŠµë‹ˆë‹¤.
""")

# --- ì˜ˆì œ DataFrame ìƒì„± (ì „ì²˜ë¦¬ ì‹œì—°ìš©) ---
@st.cache_data # ë°ì´í„°í”„ë ˆì„ ìƒì„±ì„ ìºì‹±í•˜ì—¬ ë°˜ë³µ ì‹¤í–‰ ë°©ì§€
def load_preprocessing_data():
    """ì „ì²˜ë¦¬ ì˜ˆì œìš© ìƒ˜í”Œ ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    data = {
        'Age': [25, 30, np.nan, 35, 22, 28, 40, np.nan, 33, 50],
        'Salary': [50000, 60000, 75000, np.nan, 45000, 55000, 120000, 85000, 70000, 150000],
        'City': ['New York', 'Paris', 'London', 'Tokyo', 'Seoul', 'Berlin', 'Paris', 'New York', 'London', 'Tokyo'],
        'Gender': ['Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Female'],
        'Experience': [2, 5, 3, 8, 1, 4, 15, 6, 7, 20],
        'Purchased': [0, 1, 1, 0, 1, 0, 1, 1, 0, 1] # Target variable
    }
    df = pd.DataFrame(data)
    return df

sample_df_prep = load_preprocessing_data()

st.subheader("ë°ì´í„° ì „ì²˜ë¦¬ ì˜ˆì œìš© DataFrame")
if st.checkbox("ì „ì²˜ë¦¬ ì˜ˆì œ DataFrame ë³´ê¸°", key="show_prep_base_df_page_2"): # í˜ì´ì§€ë³„ í‚¤ êµ¬ë¶„
    st.dataframe(sample_df_prep)
    st.write("ê²°ì¸¡ì¹˜ í™•ì¸ (ì—´ ë³„ ê°œìˆ˜):")
    st.dataframe(sample_df_prep.isnull().sum().rename("ê²°ì¸¡ì¹˜ ìˆ˜"))

st.markdown("---")

# --- 2.1 ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Handling Missing Values) ---
st.subheader("2.1 ê²°ì¸¡ì¹˜ ì²˜ë¦¬")
st.markdown("""
ê²°ì¸¡ì¹˜ëŠ” ëª¨ë¸ ì„±ëŠ¥ì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì ì ˆíˆ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
- **ì œê±°:** ê²°ì¸¡ì¹˜ê°€ í¬í•¨ëœ í–‰ì´ë‚˜ ì—´ì„ ì‚­ì œí•©ë‹ˆë‹¤ (`dropna()`). ë°ì´í„° ì†ì‹¤ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **ëŒ€ì¹˜ (Imputation):** ê²°ì¸¡ì¹˜ë¥¼ íŠ¹ì • ê°’(ì˜ˆ: í‰ê· , ì¤‘ì•™ê°’, ìµœë¹ˆê°’)ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤. `SimpleImputer`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""")
code_imputation = """
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

# ì˜ˆì œ ë°ì´í„° (sample_df_prepì˜ 'Age', 'Salary' ì‚¬ìš© ê°€ì •)
# df_to_impute = sample_df_prep[['Age', 'Salary']].copy()

# í‰ê· ê°’ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ëŒ€ì¹˜
# strategy: 'mean', 'median', 'most_frequent', 'constant' (fill_value ì§€ì • í•„ìš”)
imputer_mean = SimpleImputer(strategy='mean')

# fit_transformì€ í•™ìŠµ(í‰ê·  ê³„ì‚° ë“±)ê³¼ ë³€í™˜ì„ ë™ì‹œì— ìˆ˜í–‰
# ê²°ê³¼ëŠ” NumPy ë°°ì—´ì´ë¯€ë¡œ ë‹¤ì‹œ DataFrameìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ê²ƒì´ ì¢‹ìŒ
# df_to_impute[['Age', 'Salary']] = imputer_mean.fit_transform(df_to_impute[['Age', 'Salary']])
# print("í‰ê· ê°’ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ëŒ€ì¹˜ í›„:\\n", df_to_impute)

# ë‹¤ë¥¸ ì˜ˆ: 'Salary'ëŠ” ì¤‘ì•™ê°’ìœ¼ë¡œ, 'Age'ëŠ” ìƒìˆ˜(ì˜ˆ: -1)ë¡œ ëŒ€ì¹˜
# imputer_median_salary = SimpleImputer(strategy='median')
# df_to_impute['Salary'] = imputer_median_salary.fit_transform(df_to_impute[['Salary']])

# imputer_constant_age = SimpleImputer(strategy='constant', fill_value=-1)
# df_to_impute['Age'] = imputer_constant_age.fit_transform(df_to_impute[['Age']])
"""
st.code(code_imputation, language='python')

if st.checkbox("ê²°ì¸¡ì¹˜ ëŒ€ì¹˜ (`SimpleImputer`) ì˜ˆì‹œ ë³´ê¸°", key="imputation_page_2"):
    df_impute_ex = sample_df_prep[['Age', 'Salary']].copy() # ì›ë³¸ ë³€ê²½ ë°©ì§€
    st.write("ê²°ì¸¡ì¹˜ ëŒ€ì¹˜ ì „ ('Age', 'Salary' ì»¬ëŸ¼):")
    st.dataframe(df_impute_ex)

    # í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì¹˜
    imputer_mean_ex = SimpleImputer(strategy='mean')
    df_impute_ex[['Age', 'Salary']] = imputer_mean_ex.fit_transform(df_impute_ex[['Age', 'Salary']])
    
    st.write("í‰ê· ê°’ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ëŒ€ì¹˜ í›„:")
    st.dataframe(df_impute_ex.round(2)) # ì†Œìˆ˜ì  2ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼í•˜ì—¬ í‘œì‹œ
    # imputer_mean_ex.statistics_ëŠ” ê° ì—´ì— ëŒ€í•´ ê³„ì‚°ëœ í‰ê· ê°’ì„ ë‹´ê³  ìˆìŒ
    st.caption(f"Age í‰ê· : {imputer_mean_ex.statistics_[0]:.2f}, Salary í‰ê· : {imputer_mean_ex.statistics_[1]:.2f} (ì´ ê°’ë“¤ë¡œ NaNì´ ì±„ì›Œì§)")

    # ì¤‘ì•™ê°’/ìµœë¹ˆê°’ ëŒ€ì¹˜ ì˜ˆì‹œ (ìƒˆë¡œìš´ DataFrameì—ì„œ)
    df_impute_median_mode = sample_df_prep[['Age', 'Salary']].copy()
    imputer_median = SimpleImputer(strategy='median')
    imputer_most_frequent = SimpleImputer(strategy='most_frequent')

    df_impute_median_mode['Age_median_imputed'] = imputer_median.fit_transform(df_impute_median_mode[['Age']])
    df_impute_median_mode['Salary_mode_imputed'] = imputer_most_frequent.fit_transform(df_impute_median_mode[['Salary']])

    st.write("AgeëŠ” ì¤‘ì•™ê°’, SalaryëŠ” ìµœë¹ˆê°’(ì—¬ê¸°ì„œëŠ” ë°ì´í„° íŠ¹ì„±ìƒ í‰ê· /ì¤‘ì•™ê°’ê³¼ ìœ ì‚¬í•  ìˆ˜ ìˆìŒ)ìœ¼ë¡œ ëŒ€ì¹˜:")
    st.dataframe(df_impute_median_mode[['Age', 'Age_median_imputed', 'Salary', 'Salary_mode_imputed']].round(2))


st.markdown("---")

# --- 2.2 íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ (Feature Scaling) ---
st.subheader("2.2 íŠ¹ì„± ìŠ¤ì¼€ì¼ë§")
st.markdown("""
ì„œë¡œ ë‹¤ë¥¸ ë²”ìœ„ì˜ ê°’ì„ ê°€ì§„ íŠ¹ì„±ë“¤ì´ ëª¨ë¸ í•™ìŠµì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê· ë“±í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ ìŠ¤ì¼€ì¼ì„ ì¡°ì •í•©ë‹ˆë‹¤.
ê±°ë¦¬ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜(KNN, SVM)ì´ë‚˜ ê²½ì‚¬ í•˜ê°•ë²• ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜(ì„ í˜• íšŒê·€, ë¡œì§€ìŠ¤í‹± íšŒê·€, ì‹ ê²½ë§)ì— íŠ¹íˆ ì¤‘ìš”í•©ë‹ˆë‹¤.
- **`StandardScaler` (í‘œì¤€í™”):** ê° íŠ¹ì„±ì˜ í‰ê· ì„ 0, í‘œì¤€í¸ì°¨ë¥¼ 1ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ($z = (x - \mu) / \sigma$)
- **`MinMaxScaler` (ì •ê·œí™”):** ê° íŠ¹ì„±ì˜ ê°’ì„ 0ê³¼ 1 ì‚¬ì´ì˜ ë²”ìœ„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ($x' = (x - min) / (max - min)$)
- **`RobustScaler`:** ì¤‘ì•™ê°’(median)ê³¼ ì‚¬ë¶„ìœ„ ë²”ìœ„(IQR)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤ì¼€ì¼ë§í•©ë‹ˆë‹¤. ì´ìƒì¹˜(outlier)ì— ëœ ë¯¼ê°í•©ë‹ˆë‹¤.
""")
code_scaling = """
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ë¥¼ ë¨¼ì € í•´ì•¼ í•¨

# sample_df_prepì˜ 'Age', 'Salary', 'Experience' ì‚¬ìš© ê°€ì •
# numeric_cols = ['Age', 'Salary', 'Experience']
# df_numeric = sample_df_prep[numeric_cols].copy()

# ê²°ì¸¡ì¹˜ ëŒ€ì¹˜ (ì˜ˆ: ì¤‘ì•™ê°’)
# imputer = SimpleImputer(strategy='median')
# df_numeric_imputed = pd.DataFrame(imputer.fit_transform(df_numeric), columns=numeric_cols)

# StandardScaler ì‚¬ìš©
# scaler_standard = StandardScaler()
# scaled_standard_features = scaler_standard.fit_transform(df_numeric_imputed)
# df_standard_scaled = pd.DataFrame(scaled_standard_features, columns=numeric_cols)
# print("StandardScaler ì ìš© í›„:\\n", df_standard_scaled.head().round(2))

# MinMaxScaler ì‚¬ìš©
# scaler_minmax = MinMaxScaler()
# scaled_minmax_features = scaler_minmax.fit_transform(df_numeric_imputed)
# df_minmax_scaled = pd.DataFrame(scaled_minmax_features, columns=numeric_cols)
# print("\\nMinMaxScaler ì ìš© í›„:\\n", df_minmax_scaled.head().round(2))
"""
st.code(code_scaling, language='python')

if st.checkbox("íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ ì˜ˆì‹œ ë³´ê¸°", key="scaling_page_2"):
    # ê²°ì¸¡ì¹˜ë¥¼ ë¨¼ì € ì±„ìš´ í›„ ìŠ¤ì¼€ì¼ë§ ì ìš©
    numeric_cols_for_scaling = ['Age', 'Salary', 'Experience']
    df_scale_ex_orig = sample_df_prep[numeric_cols_for_scaling].copy()
    
    imputer_for_scaling = SimpleImputer(strategy='median') # ì¤‘ì•™ê°’ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ëŒ€ì¹˜
    df_scale_ex_imputed = pd.DataFrame(
        imputer_for_scaling.fit_transform(df_scale_ex_orig),
        columns=numeric_cols_for_scaling
    )
    st.write("ìŠ¤ì¼€ì¼ë§ ì „ ë°ì´í„° (ê²°ì¸¡ì¹˜ëŠ” ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì¹˜ë¨):")
    st.dataframe(df_scale_ex_imputed.round(2))

    # StandardScaler
    scaler_std = StandardScaler()
    scaled_std_data = scaler_std.fit_transform(df_scale_ex_imputed) # NumPy ë°°ì—´ ë°˜í™˜
    df_scaled_std_result = pd.DataFrame(scaled_std_data, columns=numeric_cols_for_scaling)
    st.write("`StandardScaler` ì ìš© í›„ (í‰ê·  0, í‘œì¤€í¸ì°¨ 1 ê·¼ì‚¬):")
    st.dataframe(df_scaled_std_result.round(2))
    st.caption(f"StandardScaler ì ìš© í›„ í‰ê· : {df_scaled_std_result.mean().round(2).to_dict()}, í‘œì¤€í¸ì°¨: {df_scaled_std_result.std().round(2).to_dict()}")


    # MinMaxScaler
    scaler_mm = MinMaxScaler()
    scaled_mm_data = scaler_mm.fit_transform(df_scale_ex_imputed) # NumPy ë°°ì—´ ë°˜í™˜
    df_scaled_mm_result = pd.DataFrame(scaled_mm_data, columns=numeric_cols_for_scaling)
    st.write("`MinMaxScaler` ì ìš© í›„ (0ê³¼ 1 ì‚¬ì´ë¡œ ìŠ¤ì¼€ì¼ë§):")
    st.dataframe(df_scaled_mm_result.round(2))
    st.caption(f"MinMaxScaler ì ìš© í›„ ìµœì†Œê°’: {df_scaled_mm_result.min().round(2).to_dict()}, ìµœëŒ€ê°’: {df_scaled_mm_result.max().round(2).to_dict()}")


st.markdown("---")

# --- 2.3 ë²”ì£¼í˜• ë°ì´í„° ì¸ì½”ë”© (Categorical Data Encoding) ---
st.subheader("2.3 ë²”ì£¼í˜• ë°ì´í„° ì¸ì½”ë”©")
st.markdown("""
ëŒ€ë¶€ë¶„ì˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ ìˆ«ìí˜• ì…ë ¥ì„ ê°€ì •í•˜ë¯€ë¡œ, ë¬¸ìì—´ë¡œ ëœ ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
- **`LabelEncoder`:** ë²”ì£¼í˜• ê°’ì„ 0ë¶€í„° (í´ë˜ìŠ¤ ìˆ˜ - 1)ê¹Œì§€ì˜ ì •ìˆ˜ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì£¼ë¡œ íƒ€ê²Ÿ ë³€ìˆ˜(y) ì¸ì½”ë”©ì— ì‚¬ìš©ë˜ê±°ë‚˜, ìˆœì„œê°€ ìˆëŠ” íŠ¹ì„±ì— ì œí•œì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. (ì£¼ì˜: íŠ¹ì„± ê°„ ìˆœì„œ ê´€ê³„ê°€ ì—†ëŠ” ëª…ëª©í˜• íŠ¹ì„±ì— ì‚¬ìš© ì‹œ ëª¨ë¸ì´ ì˜ëª»ëœ ìˆœì„œë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŒ)
- **`OneHotEncoder`:** ê° ë²”ì£¼ë¥¼ ìƒˆë¡œìš´ ì´ì§„(0 ë˜ëŠ” 1) íŠ¹ì„±(ì»¬ëŸ¼)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ë”ë¯¸ ë³€ìˆ˜(dummy variable)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ëª…ëª©í˜• íŠ¹ì„±ì— ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
- **`pandas.get_dummies()`:** Pandasì—ì„œ ì›-í•« ì¸ì½”ë”©ì„ ì‰½ê²Œ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
""")
code_encoding = """
import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
# sample_df_prepì˜ 'City', 'Gender' ì‚¬ìš© ê°€ì •
# df_to_encode = sample_df_prep[['City', 'Gender']].copy()

# LabelEncoder ì‚¬ìš© (ì˜ˆ: 'Gender' ì»¬ëŸ¼)
# le = LabelEncoder()
# df_to_encode['Gender_LabelEncoded'] = le.fit_transform(df_to_encode['Gender'])
# print("LabelEncoder ì ìš© í›„ ('Gender'):\\n", df_to_encode[['Gender', 'Gender_LabelEncoded']].head())
# print("LabelEncoder í´ë˜ìŠ¤:", le.classes_) # ['Female', 'Male'] -> 0, 1 ë§¤í•‘ í™•ì¸

# OneHotEncoder ì‚¬ìš© (ì˜ˆ: 'City' ì»¬ëŸ¼)
# ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # sparse_output=FalseëŠ” NumPy ë°°ì—´ ë°˜í™˜
                                                                # handle_unknown='ignore'ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì²˜ìŒ ë³¸ ë²”ì£¼ ë‚˜ì˜¤ë©´ ëª¨ë‘ 0ìœ¼ë¡œ ì²˜ë¦¬
# city_one_hot_encoded = ohe.fit_transform(df_to_encode[['City']]) # 2D ë°°ì—´ í˜•íƒœë¡œ ì…ë ¥
# # ìƒì„±ëœ ì»¬ëŸ¼ ì´ë¦„ ê°€ì ¸ì˜¤ê¸° (ohe.categories_ ë˜ëŠ” ohe.get_feature_names_out ì‚¬ìš©)
# city_encoded_cols = ohe.get_feature_names_out(['City']) # ì…ë ¥ íŠ¹ì„± ì´ë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆ ì»¬ëŸ¼ëª… ìƒì„±
# df_city_encoded = pd.DataFrame(city_one_hot_encoded, columns=city_encoded_cols, index=df_to_encode.index)
# # ì›ë³¸ DataFrameê³¼ ë³‘í•© (City ì—´ì€ ì œê±°)
# df_encoded_final = pd.concat([df_to_encode.drop('City', axis=1), df_city_encoded], axis=1)
# print("\\nOneHotEncoder ì ìš© í›„ ('City'):\\n", df_encoded_final.head())


# pandas.get_dummies() ì‚¬ìš© (ê°€ì¥ ê°„í¸í•œ ë°©ë²• ì¤‘ í•˜ë‚˜)
# df_original = sample_df_prep.copy()
# df_dummies = pd.get_dummies(df_original, columns=['City', 'Gender'], prefix=['CityIs', 'GenderIs'], drop_first=True, dtype=int)
# # drop_first=TrueëŠ” ë‹¤ì¤‘ê³µì„ ì„± ë°©ì§€ë¥¼ ìœ„í•´ ì²« ë²ˆì§¸ ë²”ì£¼ì— ëŒ€í•œ ë”ë¯¸ ë³€ìˆ˜ ì œê±°
# # dtype=intë¡œ ê²°ê³¼ íƒ€ì…ì„ ì •ìˆ˜ë¡œ ì§€ì •
# print("\\npd.get_dummies() ì ìš© í›„ (drop_first=True):\\n", df_dummies.head())
"""
st.code(code_encoding, language='python')

if st.checkbox("ë²”ì£¼í˜• ë°ì´í„° ì¸ì½”ë”© ì˜ˆì‹œ ë³´ê¸°", key="encoding_page_2"):
    df_encode_ex_orig = sample_df_prep[['City', 'Gender']].copy()
    st.write("ì¸ì½”ë”© ì „ ë°ì´í„°:")
    st.dataframe(df_encode_ex_orig)

    # LabelEncoder
    df_le_ex = df_encode_ex_orig.copy()
    le = LabelEncoder()
    df_le_ex['Gender_LabelEncoded'] = le.fit_transform(df_le_ex['Gender'])
    st.write("`LabelEncoder` ì ìš© í›„ ('Gender'):")
    st.dataframe(df_le_ex[['Gender', 'Gender_LabelEncoded']])
    st.caption(f"LabelEncoder í´ë˜ìŠ¤ ë§¤í•‘ (`le.classes_`): `{list(le.classes_)}` -> `{list(range(len(le.classes_)))}`")

    # OneHotEncoder
    df_ohe_ex_base = df_encode_ex_orig.copy()
    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore') 
    city_one_hot_data = ohe.fit_transform(df_ohe_ex_base[['City']]) # 2D í˜•íƒœë¡œ ì…ë ¥
    city_encoded_cols_names = ohe.get_feature_names_out(['City'])
    df_city_ohe_result = pd.DataFrame(city_one_hot_data, columns=city_encoded_cols_names, index=df_ohe_ex_base.index)
    
    st.write("`OneHotEncoder` ì ìš© í›„ ('City'):")
    st.dataframe(df_city_ohe_result)
    st.caption("`handle_unknown='ignore'`ëŠ” í…ŒìŠ¤íŠ¸ ì‹œ ì²˜ìŒ ë³´ëŠ” ë²”ì£¼ê°€ ë‚˜ì˜¤ë©´ ëª¨ë“  ì›í•«ì¸ì½”ë”© ì—´ì„ 0ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.")


    # pandas.get_dummies
    st.write("`pd.get_dummies()` ì ìš© í›„ ('City', 'Gender' ë™ì‹œ ë³€í™˜, `drop_first=True`):")
    df_get_dummies_result = pd.get_dummies(
        sample_df_prep[['City', 'Gender']], 
        columns=['City', 'Gender'], 
        prefix={'City':'City', 'Gender':'Is'}, # ì ‘ë‘ì‚¬ ì§€ì • (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ ê°€ëŠ¥)
        drop_first=True, # ë‹¤ì¤‘ê³µì„ ì„± ë°©ì§€ë¥¼ ìœ„í•´ ì²« ë²ˆì§¸ ë²”ì£¼ ì»¬ëŸ¼ ì œê±°
        dtype=int # ê²°ê³¼ íƒ€ì…ì„ ì •ìˆ˜ë¡œ
    )
    st.dataframe(df_get_dummies_result)
    st.caption("`drop_first=True`ëŠ” ë²”ì£¼ê°€ Nê°œì¼ ë•Œ N-1ê°œì˜ ë”ë¯¸ ë³€ìˆ˜ë§Œ ìƒì„±í•˜ì—¬ ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œë¥¼ ì™„í™”í•©ë‹ˆë‹¤.")


st.markdown("---")

# --- 2.4 ë°ì´í„° ë¶„í•  (Train-Test Split) ---
st.subheader("2.4 ë°ì´í„° ë¶„í•  (`train_test_split`)")
st.markdown("""
ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³  í‰ê°€í•˜ê¸° ìœ„í•´ ì „ì²´ ë°ì´í„°ì…‹ì„ í›ˆë ¨ ë°ì´í„°ì…‹(training set)ê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹(test set)ìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
- ëª¨ë¸ì€ í›ˆë ¨ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤.
- í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤ (ëª¨ë¸ì´ ë³´ì§€ ëª»í•œ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì¼ë°˜í™” ì„±ëŠ¥ ì¸¡ì •).
`sklearn.model_selection.train_test_split()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- `*arrays`: ë¶„í• í•  íŠ¹ì„± ë°ì´í„°(X)ì™€ ë ˆì´ë¸” ë°ì´í„°(y).
- `test_size`: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì˜ ë¹„ìœ¨ (0.0 ~ 1.0 ì‚¬ì´ ì‹¤ìˆ˜) ë˜ëŠ” ê°œìˆ˜ (ì •ìˆ˜). (ê¸°ë³¸ê°’: 0.25)
- `train_size`: í›ˆë ¨ ë°ì´í„°ì…‹ì˜ ë¹„ìœ¨ ë˜ëŠ” ê°œìˆ˜. `test_size`ì™€ ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì§€ì •.
- `random_state`: ë‚œìˆ˜ ì‹œë“œ. ë™ì¼í•œ ì‹œë“œë¥¼ ì‚¬ìš©í•˜ë©´ í•­ìƒ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ê°€ ë¶„í• ë˜ì–´ ê²°ê³¼ ì¬í˜„ ê°€ëŠ¥.
- `stratify`: (ë¶„ë¥˜ ë¬¸ì œì—ì„œ) ì§€ì •ëœ ë°°ì—´(ë³´í†µ y ë ˆì´ë¸”)ì˜ í´ë˜ìŠ¤ ë¹„ìœ¨ì„ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ëª¨ë‘ì— ìœ ì‚¬í•˜ê²Œ ìœ ì§€. ë¶ˆê· í˜• ë°ì´í„°ì…‹ì— ìœ ìš©.
""")
code_train_test_split = """
import pandas as pd
from sklearn.model_selection import train_test_split
# sample_df_prep DataFrameì´ ìˆë‹¤ê³  ê°€ì • (X: íŠ¹ì„±ë“¤, y: 'Purchased' ì»¬ëŸ¼)

# # ì „ì²˜ë¦¬ ê³¼ì •ì´ ì„ í–‰ë˜ì–´ì•¼ í•¨ (ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ì¸ì½”ë”© ë“±)
# df_processed = sample_df_prep.copy()
# # ì˜ˆì‹œ: ê°„ë‹¨í•œ ì „ì²˜ë¦¬
# for col in ['Age', 'Salary']: # ìˆ«ìí˜• ì»¬ëŸ¼ ê²°ì¸¡ì¹˜ í‰ê· ìœ¼ë¡œ
#     df_processed[col].fillna(df_processed[col].mean(), inplace=True)
# df_processed = pd.get_dummies(df_processed, columns=['City', 'Gender'], drop_first=True, dtype=int)

# # íŠ¹ì„±(X)ê³¼ íƒ€ê²Ÿ(y) ë¶„ë¦¬
# X = df_processed.drop('Purchased', axis=1) # 'Purchased' ì—´ì„ ì œì™¸í•œ ëª¨ë“  ì—´
# y = df_processed['Purchased']              # 'Purchased' ì—´

# # í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
# X_train, X_test, y_train, y_test = train_test_split(
#     X, y,
#     test_size=0.3,     # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ 30%
#     random_state=42,   # ê²°ê³¼ ì¬í˜„ì„ ìœ„í•œ ì‹œë“œ
#     stratify=y         # yì˜ í´ë˜ìŠ¤ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©° ë¶„í•  (ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì¤‘ìš”)
# )

# print("í›ˆë ¨ ë°ì´í„° X í˜•íƒœ:", X_train.shape)
# print("í…ŒìŠ¤íŠ¸ ë°ì´í„° X í˜•íƒœ:", X_test.shape)
# print("í›ˆë ¨ ë°ì´í„° y í˜•íƒœ:", y_train.shape)
# print("í…ŒìŠ¤íŠ¸ ë°ì´í„° y í˜•íƒœ:", y_test.shape)
# print("\\ní›ˆë ¨ ë°ì´í„° y í´ë˜ìŠ¤ ë¹„ìœ¨:\\n", y_train.value_counts(normalize=True).round(2))
# print("í…ŒìŠ¤íŠ¸ ë°ì´í„° y í´ë˜ìŠ¤ ë¹„ìœ¨:\\n", y_test.value_counts(normalize=True).round(2))
"""
st.code(code_train_test_split, language='python')

if st.checkbox("`train_test_split` ì˜ˆì‹œ ë³´ê¸°", key="train_test_split_page_2"):
    df_split_ex_orig = sample_df_prep.copy()
    
    # ì „ì²˜ë¦¬ (ì˜ˆì‹œ: ê²°ì¸¡ì¹˜ ì±„ìš°ê³  ë²”ì£¼í˜• ì¸ì½”ë”©)
    # ìˆ«ìí˜• ì»¬ëŸ¼ ê²°ì¸¡ì¹˜: ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì¹˜
    for col in ['Age', 'Salary', 'Experience']: 
        df_split_ex_orig[col].fillna(df_split_ex_orig[col].median(), inplace=True)
    # ë²”ì£¼í˜• ì»¬ëŸ¼: ì›-í•« ì¸ì½”ë”©
    df_split_ex_processed = pd.get_dummies(df_split_ex_orig, columns=['City', 'Gender'], drop_first=True, dtype=int)

    X = df_split_ex_processed.drop('Purchased', axis=1)
    y = df_split_ex_processed['Purchased'] # íƒ€ê²Ÿ ë³€ìˆ˜
    
    st.write("ì „ì²˜ë¦¬ ì™„ë£Œëœ íŠ¹ì„± ë°ì´í„° `X` (ìƒìœ„ 5í–‰):")
    st.dataframe(X.head())
    st.write("íƒ€ê²Ÿ ë°ì´í„° `y` (ìƒìœ„ 5í–‰):")
    st.dataframe(y.head())

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=0.3,  # í…ŒìŠ¤íŠ¸ì…‹ ë¹„ìœ¨ 30%
        random_state=123, # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ
        stratify=y      # ë¶„ë¥˜ ë¬¸ì œì—ì„œ íƒ€ê²Ÿ ë³€ìˆ˜ì˜ í´ë˜ìŠ¤ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©° ë¶„í• 
    )
    st.markdown("#### ë¶„í•  ê²°ê³¼:")
    st.write(f"- `X_train` í˜•íƒœ: `{X_train.shape}`")
    st.write(f"- `X_test` í˜•íƒœ: `{X_test.shape}`")
    st.write(f"- `y_train` í˜•íƒœ: `{y_train.shape}`")
    st.write(f"- `y_test` í˜•íƒœ: `{y_test.shape}`")
    
    st.write("`y_train` í´ë˜ìŠ¤ ë¶„í¬ (ë¹„ìœ¨):")
    st.dataframe(y_train.value_counts(normalize=True).rename("ë¹„ìœ¨").round(3))
    st.write("`y_test` í´ë˜ìŠ¤ ë¶„í¬ (ë¹„ìœ¨) (stratify=yë¡œ ì¸í•´ y_trainê³¼ ìœ ì‚¬):")
    st.dataframe(y_test.value_counts(normalize=True).rename("ë¹„ìœ¨").round(3))

st.markdown("---")
st.markdown("ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ë°˜ë³µì ì´ê³  ì‹¤í—˜ì ì¸ ê³¼ì •ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ì™€ ëª¨ë¸ì— ê°€ì¥ ì í•©í•œ ì „ì²˜ë¦¬ ë°©ë²•ì„ ì°¾ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ê¸°ë²•ì„ ì‹œë„í•´ë³´ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. Scikit-learnì˜ `Pipeline`ì„ ì‚¬ìš©í•˜ë©´ ì´ëŸ¬í•œ ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì´í›„ í˜ì´ì§€ì—ì„œ ì†Œê°œ).")