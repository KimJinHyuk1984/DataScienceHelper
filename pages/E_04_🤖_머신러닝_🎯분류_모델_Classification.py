# pages/4_ğŸ¯_ë¶„ë¥˜_ëª¨ë¸_Classification.py
import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.impute import SimpleImputer # ë°ì´í„°ì…‹ì— ë”°ë¼ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ìš©
from utils.utils_ml import get_dataset, display_classification_metrics # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš©

st.header("4. ë¶„ë¥˜ ëª¨ë¸ (Classification Models)")
st.markdown("""
ë¶„ë¥˜ëŠ” ë¯¸ë¦¬ ì •ì˜ëœ ì—¬ëŸ¬ ë²”ì£¼(í´ë˜ìŠ¤ ë˜ëŠ” ë ˆì´ë¸”) ì¤‘ í•˜ë‚˜ë¡œ ì…ë ¥ ë°ì´í„°ë¥¼ í• ë‹¹í•˜ëŠ” ì§€ë„ í•™ìŠµ ê¸°ë²•ì…ë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, ì´ë©”ì¼ì´ ìŠ¤íŒ¸ì¸ì§€ ì•„ë‹Œì§€, ì´ë¯¸ì§€ê°€ ê³ ì–‘ì´ì¸ì§€ ê°•ì•„ì§€ì¸ì§€, ë˜ëŠ” ê³ ê°ì˜ ì´íƒˆ ì—¬ë¶€ ë“±ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
""")

# --- ì˜ˆì œ ë°ì´í„°ì…‹ ë¡œë“œ (ì™€ì¸ ë°ì´í„° ë˜ëŠ” ìœ ë°©ì•” ë°ì´í„°) ---
st.subheader("ì™€ì¸ ë˜ëŠ” ìœ ë°©ì•” ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ ë¶„ë¥˜ ì˜ˆì œ")
dataset_options = ["wine", "breast_cancer", "iris"] # iris ì¶”ê°€
chosen_dataset_cls = st.selectbox(
    "ë¶„ë¥˜ ì˜ˆì œì— ì‚¬ìš©í•  ë°ì´í„°ì…‹ì„ ì„ íƒí•˜ì„¸ìš”:",
    dataset_options,
    key="cls_dataset_selector"
)

try:
    df_cls, target_names_cls, feature_names_cls = get_dataset(chosen_dataset_cls)
except Exception as e:
    st.error(f"{chosen_dataset_cls} ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    st.info("Scikit-learn ë°ì´í„°ì…‹ ì„œë²„ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    df_cls = None

if df_cls is not None:
    if st.checkbox(f"{chosen_dataset_cls} ë°ì´í„°ì…‹ ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 5í–‰)", key=f"show_{chosen_dataset_cls}_df_page_4"):
        st.dataframe(df_cls.head())
        st.write(f"ë°ì´í„° í˜•íƒœ: {df_cls.shape}")
        st.write("íŠ¹ì„± (Features):", feature_names_cls)
        st.write("íƒ€ê²Ÿ (Target) í´ë˜ìŠ¤ ì´ë¦„:", target_names_cls if target_names_cls is not None else "ë‹¨ì¼ íƒ€ê²Ÿ (íšŒê·€) ë˜ëŠ” ì´ë¦„ ì—†ìŒ")
        st.write("íƒ€ê²Ÿ ë³€ìˆ˜ ê³ ìœ ê°’:", df_cls['target'].unique())
        st.write("ê²°ì¸¡ì¹˜ í™•ì¸:")
        st.dataframe(df_cls.isnull().sum().rename("ê²°ì¸¡ì¹˜ ìˆ˜"))


    st.markdown("---")

    # --- 4.1 ë°ì´í„° ì „ì²˜ë¦¬ (ë¶„ë¥˜ìš©) ---
    st.subheader("4.1 ë°ì´í„° ì „ì²˜ë¦¬ (ë¶„ë¥˜ìš©)")
    st.markdown("""
    ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì „, ê²°ì¸¡ì¹˜ ì²˜ë¦¬, íŠ¹ì„± ìŠ¤ì¼€ì¼ë§, ë°ì´í„° ë¶„í•  ë“±ì˜ ì „ì²˜ë¦¬ ê³¼ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ë¬¸ìì—´ì¸ ê²½ìš°, `LabelEncoder` ë“±ì„ ì‚¬ìš©í•˜ì—¬ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
    """)

    # íŠ¹ì„±(X)ê³¼ íƒ€ê²Ÿ(y) ë¶„ë¦¬
    X_cls = df_cls.drop('target', axis=1)
    y_cls = df_cls['target']

    # (ì°¸ê³ ) ë§Œì•½ y_clsê°€ ë¬¸ìì—´ ë ˆì´ë¸”ì´ë¼ë©´ LabelEncoder ì‚¬ìš©
    # le = LabelEncoder()
    # y_cls_encoded = le.fit_transform(y_cls)
    # target_names_cls = le.classes_ # ì¸ì½”ë”©ëœ í´ë˜ìŠ¤ ì´ë¦„ ì €ì¥

    # í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
    X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(
        X_cls, y_cls,
        test_size=0.3,     # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ 30%
        random_state=42,   # ê²°ê³¼ ì¬í˜„ì„ ìœ„í•œ ì‹œë“œ
        stratify=y_cls     # yì˜ í´ë˜ìŠ¤ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©° ë¶„í• 
    )

    # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ (StandardScaler ì‚¬ìš©)
    scaler_cls = StandardScaler()
    X_train_scaled_c = scaler_cls.fit_transform(X_train_c)
    X_test_scaled_c = scaler_cls.transform(X_test_c)

    if st.checkbox("ì „ì²˜ë¦¬ëœ ë¶„ë¥˜ ë°ì´í„° ì¼ë¶€ í™•ì¸", key="show_preprocessed_cls_data_page_4"):
        st.write("í›ˆë ¨ìš© íŠ¹ì„± ë°ì´í„° (ìŠ¤ì¼€ì¼ë§ í›„, ìƒìœ„ 3í–‰):")
        st.dataframe(pd.DataFrame(X_train_scaled_c, columns=X_cls.columns).head(3).round(3))
        st.write("í…ŒìŠ¤íŠ¸ìš© íƒ€ê²Ÿ ë°ì´í„° (ìƒìœ„ 10ê°œ):", y_test_c.head(10).values)


    st.markdown("---")

    # --- 4.2 ë¡œì§€ìŠ¤í‹± íšŒê·€ (Logistic Regression) ---
    st.subheader("4.2 ë¡œì§€ìŠ¤í‹± íšŒê·€ (`LogisticRegression`)")
    st.markdown("""
    ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ì´ë¦„ì— 'íšŒê·€'ê°€ ë“¤ì–´ê°€ì§€ë§Œ, ì‹¤ì œë¡œëŠ” **ë¶„ë¥˜** ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.
    ì‹œê·¸ëª¨ì´ë“œ(Sigmoid) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ ì˜ˆì¸¡í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    ì´ì§„ ë¶„ë¥˜ì— ì£¼ë¡œ ì‚¬ìš©ë˜ì§€ë§Œ, ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ë¡œ í™•ì¥ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì˜ˆ: OvR - One-vs-Rest ë°©ì‹).
    ì„ í˜• ëª¨ë¸ì˜ ì¼ì¢…ìœ¼ë¡œ í•´ì„ì´ ìš©ì´í•˜ê³ , ë¹„êµì  í•™ìŠµ ì†ë„ê°€ ë¹ ë¦…ë‹ˆë‹¤.
    ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°: `C` (ê·œì œ ê°•ë„ì˜ ì—­ìˆ˜, ì‘ì„ìˆ˜ë¡ ê°•í•œ ê·œì œ), `penalty` (`'l1'`, `'l2'`, `'elasticnet'`).
    """)
    code_logistic_regression = """
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
# from utils_ml import display_classification_metrics (í‰ê°€ìš©)

# # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (X_train_scaled_c, X_test_scaled_c, y_train_c, y_test_c ì¤€ë¹„ ê°€ì •)
# # ... (ìœ„ì˜ ì „ì²˜ë¦¬ ì½”ë“œì™€ ë™ì¼) ...

# 1. ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ ê°ì²´ ìƒì„±
log_reg_model = LogisticRegression(solver='liblinear', random_state=42) # solverëŠ” ë°ì´í„°ì…‹ í¬ê¸°ë‚˜ penaltyì— ë”°ë¼ ì„ íƒ

# 2. ëª¨ë¸ í•™ìŠµ
log_reg_model.fit(X_train_scaled_c, y_train_c)

# 3. í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡
y_pred_log_reg = log_reg_model.predict(X_test_scaled_c)
# y_pred_proba_log_reg = log_reg_model.predict_proba(X_test_scaled_c) # ê° í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ í™•ë¥ 

# 4. ëª¨ë¸ í‰ê°€
# display_classification_metrics(y_test_c, y_pred_log_reg, target_names=target_names_cls, title="ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í‰ê°€")
    """
    st.code(code_logistic_regression, language='python')

    if st.checkbox("ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ ì‹¤í–‰ ë° í‰ê°€ ë³´ê¸°", key="logistic_regression_page_4"):
        st.markdown("#### ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡")
        # solver='liblinear'ëŠ” ì‘ì€ ë°ì´í„°ì…‹ì— ì í•©í•˜ê³  L1, L2 ê·œì œ ëª¨ë‘ ì§€ì›
        # ë‹¤ì¤‘ í´ë˜ìŠ¤ ê²½ìš° solver='lbfgs'(ê¸°ë³¸ê°’) ë˜ëŠ” 'saga' ë“±ì´ ì‚¬ìš©ë¨
        solver_option = 'liblinear' if len(df_cls['target'].unique()) == 2 else 'lbfgs'

        log_reg_model_ex = LogisticRegression(solver=solver_option, random_state=42, max_iter=200) # max_iter ì¦ê°€
        log_reg_model_ex.fit(X_train_scaled_c, y_train_c)
        y_pred_log_reg_ex = log_reg_model_ex.predict(X_test_scaled_c)
        
        display_classification_metrics(y_test_c, y_pred_log_reg_ex, target_names=target_names_cls, title="ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í‰ê°€ ê²°ê³¼")
        
        if hasattr(log_reg_model_ex, 'coef_'):
            st.write("**ëª¨ë¸ ê³„ìˆ˜ (Coefficients):**")
            # ì´ì§„ ë¶„ë¥˜ì˜ ê²½ìš° coef_ëŠ” (1, n_features), ë‹¤ì¤‘ í´ë˜ìŠ¤ëŠ” (n_classes, n_features)
            if log_reg_model_ex.coef_.shape[0] == 1: # ì´ì§„ ë¶„ë¥˜
                 coef_df_log = pd.DataFrame({'Feature': X_cls.columns, 'Coefficient': log_reg_model_ex.coef_[0]}).round(4)
            else: # ë‹¤ì¤‘ í´ë˜ìŠ¤
                coef_df_log = pd.DataFrame(log_reg_model_ex.coef_.T, index=X_cls.columns, columns=[f'Class_{i}' for i in log_reg_model_ex.classes_]).round(4)
            st.dataframe(coef_df_log)

    st.markdown("---")

    # --- 4.3 K-ìµœê·¼ì ‘ ì´ì›ƒ (K-Nearest Neighbors, KNN) ---
    st.subheader("4.3 K-ìµœê·¼ì ‘ ì´ì›ƒ (`KNeighborsClassifier`)")
    st.markdown("""
    KNNì€ ìƒˆë¡œìš´ ë°ì´í„° í¬ì¸íŠ¸ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê°€ì¥ ê°€ê¹Œìš´ `K`ê°œì˜ í›ˆë ¨ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ì°¾ì•„ ë‹¤ìˆ˜ê²° ì›ì¹™ì— ë”°ë¼ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¹„ëª¨ìˆ˜ì (non-parametric) ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.
    - ë‹¨ìˆœí•˜ê³  ì§ê´€ì ì´ì§€ë§Œ, ë°ì´í„°ê°€ ë§ì„ ê²½ìš° ì˜ˆì¸¡ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆê³ , íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ì— ë¯¼ê°í•©ë‹ˆë‹¤.
    - `K` ê°’ ì„ íƒì´ ì¤‘ìš”í•˜ë©°, ë„ˆë¬´ ì‘ìœ¼ë©´ ë…¸ì´ì¦ˆì— ë¯¼ê°í•˜ê³ , ë„ˆë¬´ í¬ë©´ ê²½ê³„ê°€ ëª¨í˜¸í•´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°: `n_neighbors` (K ê°’), `weights` (`'uniform'`, `'distance'`), `metric` (ê±°ë¦¬ ì¸¡ì • ë°©ì‹, ì˜ˆ: `'minkowski'`, `'euclidean'`).
    """)
    code_knn = """
from sklearn.neighbors import KNeighborsClassifier

# # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (X_train_scaled_c, X_test_scaled_c, y_train_c, y_test_c ì¤€ë¹„ ê°€ì •)

# 1. KNN ëª¨ë¸ ê°ì²´ ìƒì„±
knn_model = KNeighborsClassifier(n_neighbors=5) # K=5ë¡œ ì„¤ì •

# 2. ëª¨ë¸ í•™ìŠµ (KNNì€ ì‹¤ì œë¡œ í•™ìŠµ ë‹¨ê³„ì—ì„œ ë§ì€ ê³„ì‚°ì„ í•˜ì§€ ì•Šê³  ë°ì´í„°ë¥¼ ì €ì¥)
knn_model.fit(X_train_scaled_c, y_train_c)

# 3. í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡
y_pred_knn = knn_model.predict(X_test_scaled_c)

# 4. ëª¨ë¸ í‰ê°€
# display_classification_metrics(y_test_c, y_pred_knn, target_names=target_names_cls, title="KNN ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ (K=5)")
    """
    st.code(code_knn, language='python')

    if st.checkbox("KNN ë¶„ë¥˜ ëª¨ë¸ ì‹¤í–‰ ë° í‰ê°€ ë³´ê¸° (K=5)", key="knn_page_4"):
        st.markdown("#### KNN ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ (K=5)")
        knn_model_ex = KNeighborsClassifier(n_neighbors=5)
        knn_model_ex.fit(X_train_scaled_c, y_train_c)
        y_pred_knn_ex = knn_model_ex.predict(X_test_scaled_c)
        display_classification_metrics(y_test_c, y_pred_knn_ex, target_names=target_names_cls, title="KNN ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ ê²°ê³¼ (K=5)")

    st.markdown("---")

    # --- 4.4 ê²°ì • íŠ¸ë¦¬ (Decision Tree Classifier) ---
    st.subheader("4.4 ê²°ì • íŠ¸ë¦¬ (`DecisionTreeClassifier`)")
    st.markdown("""
    ê²°ì • íŠ¸ë¦¬ëŠ” ë°ì´í„°ì˜ íŠ¹ì„±ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ë¬´ê³ ê°œì™€ ê°™ì€ ì§ˆë¬¸ì„ ì—°ì†ì ìœ¼ë¡œ ë˜ì ¸ ë°ì´í„°ë¥¼ ë¶„í• í•˜ê³ , ê° ìµœì¢… ë¶„í•  ì˜ì—­(ë¦¬í”„ ë…¸ë“œ)ì— í•´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.
    - í•´ì„ì´ ìš©ì´í•˜ê³  ì‹œê°í™”ê°€ ê°€ëŠ¥í•˜ì§€ë§Œ, ê³¼ì í•©ë˜ê¸° ì‰¬ìš´ ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.
    - íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ì´ í•„ìˆ˜ëŠ” ì•„ë‹™ë‹ˆë‹¤.
    ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°: `criterion` (`'gini'`, `'entropy'`), `max_depth`, `min_samples_split`, `min_samples_leaf`.
    """)
    code_dt_cls = """
from sklearn.tree import DecisionTreeClassifier

# # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (X_train_c, X_test_c, y_train_c, y_test_c ì¤€ë¹„ ê°€ì • - ìŠ¤ì¼€ì¼ë§ ì•ˆëœ ë°ì´í„°ë„ ì‚¬ìš© ê°€ëŠ¥)

# 1. ê²°ì • íŠ¸ë¦¬ ëª¨ë¸ ê°ì²´ ìƒì„±
dt_cls_model = DecisionTreeClassifier(max_depth=4, random_state=42)

# 2. ëª¨ë¸ í•™ìŠµ
dt_cls_model.fit(X_train_c, y_train_c) # ìŠ¤ì¼€ì¼ë§ ì•ˆëœ ì›ë³¸ X_train_c ì‚¬ìš© ê°€ëŠ¥

# 3. í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡
y_pred_dt_cls = dt_cls_model.predict(X_test_c)

# 4. ëª¨ë¸ í‰ê°€
# display_classification_metrics(y_test_c, y_pred_dt_cls, target_names=target_names_cls, title="ê²°ì • íŠ¸ë¦¬ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ (max_depth=4)")

# íŠ¹ì„± ì¤‘ìš”ë„ í™•ì¸
# feature_importances = dt_cls_model.feature_importances_
# importance_df = pd.DataFrame({'Feature': X_cls.columns, 'Importance': feature_importances})
# importance_df = importance_df.sort_values(by='Importance', ascending=False)
# print("\\níŠ¹ì„± ì¤‘ìš”ë„:\\n", importance_df)
    """
    st.code(code_dt_cls, language='python')

    if st.checkbox("ê²°ì • íŠ¸ë¦¬ ë¶„ë¥˜ ëª¨ë¸ ì‹¤í–‰ ë° í‰ê°€ ë³´ê¸° (max_depth=4)", key="dt_cls_page_4"):
        st.markdown("#### ê²°ì • íŠ¸ë¦¬ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ (max_depth=4)")
        dt_cls_model_ex = DecisionTreeClassifier(max_depth=4, random_state=42)
        # íŠ¸ë¦¬ ëª¨ë¸ì€ ìŠ¤ì¼€ì¼ë§ì— ì˜í–¥ì„ ë°›ì§€ ì•Šìœ¼ë¯€ë¡œ, ìŠ¤ì¼€ì¼ë§ ì•ˆ ëœ X_train_c ì‚¬ìš© ê°€ëŠ¥
        dt_cls_model_ex.fit(X_train_c, y_train_c)
        y_pred_dt_cls_ex = dt_cls_model_ex.predict(X_test_c)
        display_classification_metrics(y_test_c, y_pred_dt_cls_ex, target_names=target_names_cls, title="ê²°ì • íŠ¸ë¦¬ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ ê²°ê³¼ (max_depth=4)")

        st.write("**íŠ¹ì„± ì¤‘ìš”ë„ (Feature Importances):**")
        importances = dt_cls_model_ex.feature_importances_
        importance_df = pd.DataFrame({'Feature': X_cls.columns, 'Importance': importances})
        importance_df = importance_df.sort_values(by='Importance', ascending=False).reset_index(drop=True)
        st.dataframe(importance_df.round(4))
        st.caption("íŠ¹ì„± ì¤‘ìš”ë„ëŠ” í•´ë‹¹ íŠ¹ì„±ì´ ëª¨ë¸ì˜ ì˜ˆì¸¡ì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.")

    st.markdown("---")
    st.markdown("""
    ì´ ì™¸ì—ë„ **ëœë¤ í¬ë ˆìŠ¤íŠ¸(`RandomForestClassifier`)**, **ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ (`SVC`)**, **ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ(`GaussianNB` ë“±)**, **ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ê³„ì—´(`GradientBoostingClassifier`, `XGBClassifier`, `LGBMClassifier`)** ë“± ë‹¤ì–‘í•œ ê°•ë ¥í•œ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ë“¤ì´ ìˆìŠµë‹ˆë‹¤.
    ê° ëª¨ë¸ì˜ ì¥ë‹¨ì ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì´í•´í•˜ê³ , êµì°¨ ê²€ì¦ ë“±ì„ í†µí•´ ë¬¸ì œì— ê°€ì¥ ì í•©í•œ ëª¨ë¸ì„ ì„ íƒí•˜ê³  íŠœë‹í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
    """)

else: # df_clsê°€ Noneì¼ ê²½ìš° (ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨)
    st.error(f"{chosen_dataset_cls} ë°ì´í„°ì…‹ì„ ë¡œë“œí•  ìˆ˜ ì—†ì–´ ë¶„ë¥˜ ëª¨ë¸ ì˜ˆì œë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.markdown("ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜, Scikit-learn ë°ì´í„°ì…‹ ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")