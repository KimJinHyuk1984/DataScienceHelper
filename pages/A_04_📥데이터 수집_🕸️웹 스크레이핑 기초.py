# pages/4_ğŸ•¸ï¸_ì›¹_ìŠ¤í¬ë ˆì´í•‘_ê¸°ì´ˆ.py
import streamlit as st
import pandas as pd
import requests # ì›¹ í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•¨
from bs4 import BeautifulSoup # HTML ë‚´ìš©ì„ íŒŒì‹±(ë¶„ì„)í•˜ê¸° ìœ„í•¨
from utils.utils_collection import display_scraped_elements # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš© (ì„ íƒ ì‚¬í•­)

st.header("4. ì›¹ ìŠ¤í¬ë ˆì´í•‘ (Web Scraping) ê¸°ì´ˆ")
st.markdown("""
ì›¹ ìŠ¤í¬ë ˆì´í•‘ì€ ì›¹ì‚¬ì´íŠ¸ì—ì„œ HTML í˜•íƒœë¡œ ì œê³µë˜ëŠ” ë°ì´í„°ë¥¼ í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.
APIê°€ ì œê³µë˜ì§€ ì•Šê±°ë‚˜, APIë¥¼ í†µí•´ ì–»ì„ ìˆ˜ ì—†ëŠ” íŠ¹ì • ì •ë³´ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì´ í˜ì´ì§€ì—ì„œ ë‹¤ë£¨ëŠ” ë‚´ìš©:**
- ì›¹ ìŠ¤í¬ë ˆì´í•‘ ì‹œ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ìœ¤ë¦¬ì  ë° ë²•ì  ê³ ë ¤ì‚¬í•­
- `requests` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ì›¹ í˜ì´ì§€ HTML ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
- `BeautifulSoup4` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ HTML ë‚´ìš© íŒŒì‹± ë° ë°ì´í„° ì¶”ì¶œ ê¸°ì´ˆ
""")

st.error("""
**âš ï¸ ê²½ê³ : ì›¹ ìŠ¤í¬ë ˆì´í•‘ì˜ ìœ¤ë¦¬ì  ë° ë²•ì  ì±…ì„ âš ï¸**

ì›¹ ìŠ¤í¬ë ˆì´í•‘ì€ ê°•ë ¥í•œ ê¸°ìˆ ì´ì§€ë§Œ, ì˜ëª» ì‚¬ìš©ë  ê²½ìš° ë²•ì  ë¬¸ì œë‚˜ ìœ¤ë¦¬ì  ë¬¸ì œë¥¼ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
**ì•„ë˜ ì‚¬í•­ì„ ë°˜ë“œì‹œ ìˆ™ì§€í•˜ê³  ì±…ì„ê° ìˆê²Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤:**

1.  **`robots.txt` í™•ì¸ ë° ì¤€ìˆ˜:** ëŒ€ë¶€ë¶„ì˜ ì›¹ì‚¬ì´íŠ¸ëŠ” ë£¨íŠ¸ ë””ë ‰í† ë¦¬(ì˜ˆ: `https://example.com/robots.txt`)ì— `robots.txt` íŒŒì¼ì„ ë‘ì–´ ì›¹ í¬ë¡¤ëŸ¬ì˜ ì ‘ê·¼ ê·œì¹™ì„ ëª…ì‹œí•©ë‹ˆë‹¤. ì´ íŒŒì¼ì—ì„œ `User-agent` ë³„ë¡œ `Allow` ë˜ëŠ” `Disallow` ëœ ê²½ë¡œë¥¼ í™•ì¸í•˜ê³ , **`Disallow`ëœ ê²½ë¡œëŠ” ì ˆëŒ€ ìŠ¤í¬ë ˆì´í•‘í•˜ì§€ ë§ˆì„¸ìš”.**
2.  **ì„œë¹„ìŠ¤ ì´ìš© ì•½ê´€ (Terms of Service, ToS) í™•ì¸:** ì›¹ì‚¬ì´íŠ¸ì˜ ì´ìš© ì•½ê´€ì—ëŠ” ë°ì´í„° ìˆ˜ì§‘ì— ê´€í•œ ì •ì±…ì´ ëª…ì‹œë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•½ê´€ì„ ìœ„ë°˜í•˜ëŠ” ìŠ¤í¬ë ˆì´í•‘ì€ ë²•ì  ì¡°ì¹˜ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3.  **ì„œë²„ ë¶€í•˜ ìµœì†Œí™”:** ì§§ì€ ì‹œê°„ ë‚´ì— ë„ˆë¬´ ë§ì€ ìš”ì²­ì„ ë³´ë‚´ë©´ ëŒ€ìƒ ì›¹ì‚¬ì´íŠ¸ ì„œë²„ì— ê³¼ë„í•œ ë¶€í•˜ë¥¼ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì„œë¹„ìŠ¤ ê±°ë¶€(DoS) ê³µê²©ìœ¼ë¡œ ê°„ì£¼ë  ìˆ˜ ìˆìœ¼ë©°, IPê°€ ì°¨ë‹¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš”ì²­ ì‚¬ì´ì— ì ì ˆí•œ ì‹œê°„ ì§€ì—°(`time.sleep()`)ì„ ë‘ê³ , í•„ìš”í•œ ìµœì†Œí•œì˜ í˜ì´ì§€ë§Œ ìš”ì²­í•˜ì„¸ìš”.
4.  **ê°œì¸ì •ë³´ ë° ì €ì‘ê¶Œ ì¡´ì¤‘:** ê°œì¸ì„ ì‹ë³„í•  ìˆ˜ ìˆëŠ” ì •ë³´ë‚˜ ì €ì‘ê¶Œì´ ìˆëŠ” ì½˜í…ì¸ ë¥¼ ë¬´ë‹¨ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê±°ë‚˜ í™œìš©í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
5.  **ë°ì´í„° í™œìš© ëª©ì  ëª…í™•í™”:** ìˆ˜ì§‘í•œ ë°ì´í„°ëŠ” í•©ë²•ì ì´ê³  ìœ¤ë¦¬ì ì¸ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

**ì´ í˜ì´ì§€ì˜ ì˜ˆì œëŠ” í•™ìŠµ ëª©ì ìœ¼ë¡œë§Œ ì œê³µë˜ë©°, ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ì— ì ìš©í•˜ê¸° ì „ì— ë°˜ë“œì‹œ í•´ë‹¹ ì‚¬ì´íŠ¸ì˜ ì •ì±…ì„ í™•ì¸í•˜ê³  í—ˆìš©ëœ ë²”ìœ„ ë‚´ì—ì„œë§Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.**
""")
st.markdown("---")

# --- 4.1 ì›¹ í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (`requests`) ---
st.subheader("4.1 ì›¹ í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (`requests`)")
st.markdown("""
ì›¹ ìŠ¤í¬ë ˆì´í•‘ì˜ ì²« ë‹¨ê³„ëŠ” ëŒ€ìƒ ì›¹ í˜ì´ì§€ì˜ HTML ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” ê²ƒì…ë‹ˆë‹¤. íŒŒì´ì¬ì˜ `requests` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ HTTP GET ìš”ì²­ì„ ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""")
code_fetch_html = """
import requests

# ìŠ¤í¬ë ˆì´í•‘í•  ëŒ€ìƒ URL (ì˜ˆì‹œ)
# ì‹¤ì œë¡œëŠ” robots.txtì™€ ToSë¥¼ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.
url = "https://books.toscrape.com/" # ìŠ¤í¬ë ˆì´í•‘ ì—°ìŠµìš©ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ì‚¬ì´íŠ¸

try:
    # HTTP GET ìš”ì²­ ë³´ë‚´ê¸°
    response = requests.get(url, timeout=5) # 5ì´ˆ íƒ€ì„ì•„ì›ƒ
    response.raise_for_status() # HTTP ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ë¥¼ ì¼ìœ¼í‚´ (4xx, 5xx ìƒíƒœ ì½”ë“œ)

    # HTML ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
    html_content = response.text
    # print(f"'{url}' ì—ì„œ HTML ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì„±ê³µ (ì¼ë¶€ë§Œ í‘œì‹œ):")
    # print(html_content[:500] + "...") # ë„ˆë¬´ ê¸¸ì–´ì„œ ì¼ë¶€ë§Œ ì¶œë ¥

except requests.exceptions.HTTPError as http_err:
    # print(f"HTTP ì—ëŸ¬ ë°œìƒ: {http_err} (ìƒíƒœ ì½”ë“œ: {response.status_code if 'response' in locals() else 'N/A'})")
    pass # Streamlitì—ì„œëŠ” st.error ì‚¬ìš©
except requests.exceptions.RequestException as req_err:
    # print(f"ìš”ì²­ ì—ëŸ¬ ë°œìƒ: {req_err}")
    pass
"""
st.code(code_fetch_html, language='python')

if st.checkbox("ì›¹ í˜ì´ì§€ HTML ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì˜ˆì‹œ ì‹¤í–‰", key="fetch_html_page_4"):
    example_url_scrape = "http://books.toscrape.com/" # ìŠ¤í¬ë ˆì´í•‘ ì—°ìŠµìš© ì‚¬ì´íŠ¸
    st.write(f"ëŒ€ìƒ URL: `{example_url_scrape}`")
    st.caption("`books.toscrape.com`ì€ ìŠ¤í¬ë ˆì´í•‘ ì—°ìŠµì„ ìœ„í•´ ì œê³µë˜ëŠ” ì›¹ì‚¬ì´íŠ¸ì…ë‹ˆë‹¤.")
    
    try:
        response_scrape = requests.get(example_url_scrape, timeout=10)
        response_scrape.raise_for_status()
        st.success(f"'{example_url_scrape}' HTML ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì„±ê³µ! (ìƒíƒœ ì½”ë“œ: {response_scrape.status_code})")
        
        with st.expander("ê°€ì ¸ì˜¨ HTML ë‚´ìš© ë³´ê¸° (ìƒìœ„ 1000ì)", expanded=False):
            st.code(response_scrape.text[:1000] + "...", language='html')
            
    except requests.exceptions.HTTPError as http_err:
        st.error(f"HTTP ì—ëŸ¬ ë°œìƒ: {http_err} (ìƒíƒœ ì½”ë“œ: {response_scrape.status_code if 'response_scrape' in locals() and hasattr(response_scrape, 'status_code') else 'N/A'})")
    except requests.exceptions.RequestException as req_err:
        st.error(f"ìš”ì²­ ì¤‘ ì—ëŸ¬ ë°œìƒ: {req_err}")

st.markdown("---")

# --- 4.2 HTML íŒŒì‹± (`BeautifulSoup4`) ---
st.subheader("4.2 HTML íŒŒì‹± (`BeautifulSoup4`)")
st.markdown("""
ê°€ì ¸ì˜¨ HTML ë¬¸ìì—´ì—ì„œ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì¶”ì¶œí•˜ë ¤ë©´ HTML êµ¬ì¡°ë¥¼ ë¶„ì„(íŒŒì‹±)í•´ì•¼ í•©ë‹ˆë‹¤. `BeautifulSoup4` (ë³´í†µ `bs4`ë¡œ ì„í¬íŠ¸)ëŠ” ì´ë¥¼ ìœ„í•œ ê°•ë ¥í•˜ê³  ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
- `pip install beautifulsoup4 lxml` ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•©ë‹ˆë‹¤. (`lxml`ì€ ë¹ ë¥´ê³  ì•ˆì •ì ì¸ HTML íŒŒì„œì…ë‹ˆë‹¤.)
- `BeautifulSoup(html_content, 'html.parser')` ë˜ëŠ” `BeautifulSoup(html_content, 'lxml')`ë¡œ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

**ì£¼ìš” ë©”ì†Œë“œ:**
- `soup.find('íƒœê·¸ì´ë¦„', attrs={'ì†ì„±ì´ë¦„': 'ì†ì„±ê°’', ...})`: íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì²« ë²ˆì§¸ ìš”ì†Œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
- `soup.find_all('íƒœê·¸ì´ë¦„', attrs={'ì†ì„±ì´ë¦„': 'ì†ì„±ê°’', ...}, limit=None)`: íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ëª¨ë“  ìš”ì†Œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì°¾ìŠµë‹ˆë‹¤.
    - `class_` íŒŒë¼ë¯¸í„°: HTML `class` ì†ì„±ìœ¼ë¡œ ê²€ìƒ‰ (ì˜ˆ: `class_='my-class'`).
- `soup.select('CSS ì„ íƒì')`: CSS ì„ íƒìë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì†Œë¥¼ ì°¾ìŠµë‹ˆë‹¤ (ë” ìœ ì—°í•œ ì„ íƒ ê°€ëŠ¥).
- `element.get_text(strip=True)`: ìš”ì†Œ ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸ ë‚´ìš©ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤ (`strip=True`ëŠ” ì•ë’¤ ê³µë°± ì œê±°).
- `element['ì†ì„±ì´ë¦„']`: ìš”ì†Œì˜ íŠ¹ì • ì†ì„± ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤ (ì˜ˆ: `a_tag['href']`ëŠ” `<a>` íƒœê·¸ì˜ `href` ì†ì„±ê°’).
""")
st.warning("ğŸ’¡ `BeautifulSoup4`ì™€ í•¨ê»˜ `lxml` íŒŒì„œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. `pip install lxml`ë¡œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

code_parse_html = """
import requests
from bs4 import BeautifulSoup

url = "https.books.toscrape.com/" # ìŠ¤í¬ë ˆì´í•‘ ì—°ìŠµìš© ì‚¬ì´íŠ¸
# html_content = requests.get(url).text # ì´ë¯¸ ê°€ì ¸ì™”ë‹¤ê³  ê°€ì •

# # ê°€ìƒì˜ HTML ë‚´ìš© (ì‹¤ì œë¡œëŠ” ìœ„ì—ì„œ ê°€ì ¸ì˜¨ html_content ì‚¬ìš©)
# html_example = \"\"\"
# <html><head><title>My Page</title></head>
# <body><h1>A Big Heading</h1>
# <p class="content-text">This is a paragraph.</p>
# <p class="content-text" id="second-p">Another paragraph.</p>
# <a href="https://example.com">Click here</a>
# <ul><li>Item 1</li><li>Item 2</li></ul></body></html>
# \"\"\"

# BeautifulSoup ê°ì²´ ìƒì„± (lxml íŒŒì„œ ì‚¬ìš© ê¶Œì¥)
# soup = BeautifulSoup(html_example, 'lxml')

# # íŠ¹ì • íƒœê·¸ ì°¾ê¸°
# title_tag = soup.find('title')
# # print(f"í˜ì´ì§€ ì œëª©: {title_tag.get_text(strip=True) if title_tag else 'ì œëª© ì—†ìŒ'}")

# h1_tag = soup.find('h1')
# # print(f"H1 íƒœê·¸ ë‚´ìš©: {h1_tag.get_text(strip=True) if h1_tag else 'H1 ì—†ìŒ'}")

# # íŠ¹ì • í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ëª¨ë“  <p> íƒœê·¸ ì°¾ê¸°
# content_paragraphs = soup.find_all('p', class_='content-text')
# # print("\\ní´ë˜ìŠ¤ê°€ 'content-text'ì¸ ë¬¸ë‹¨ë“¤:")
# # for i, p_tag in enumerate(content_paragraphs):
# #     print(f"  ë¬¸ë‹¨ {i+1}: {p_tag.get_text(strip=True)}")

# # CSS ì„ íƒì ì‚¬ìš© (ì˜ˆ: idê°€ 'second-p'ì¸ ìš”ì†Œ)
# second_p_selector = soup.select_one('#second-p') # selectëŠ” ë¦¬ìŠ¤íŠ¸ ë°˜í™˜, select_oneì€ ë‹¨ì¼ ìš”ì†Œ
# # print(f"\\nIDê°€ 'second-p'ì¸ ë¬¸ë‹¨ ë‚´ìš©: {second_p_selector.get_text(strip=True) if second_p_selector else 'ID ì—†ìŒ'}")

# # ë§í¬(<a> íƒœê·¸)ì˜ href ì†ì„± ê°’ ê°€ì ¸ì˜¤ê¸°
# link_tag = soup.find('a')
# # if link_tag and 'href' in link_tag.attrs:
# #     print(f"\\nì²« ë²ˆì§¸ ë§í¬ URL: {link_tag['href']}")
"""
st.code(code_parse_html, language='python')

if st.checkbox("`BeautifulSoup4` íŒŒì‹± ë° ë°ì´í„° ì¶”ì¶œ ì˜ˆì‹œ ì‹¤í–‰", key="parse_html_page_4"):
    scrape_target_url = "http://books.toscrape.com/"
    st.write(f"ëŒ€ìƒ URL: `{scrape_target_url}`")
    st.caption("`books.toscrape.com`ì˜ ì²« í˜ì´ì§€ì—ì„œ ì±… ì œëª©ê³¼ ê°€ê²©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.")

    try:
        response = requests.get(scrape_target_url, timeout=10)
        response.raise_for_status()
        html_to_parse = response.text
        
        soup = BeautifulSoup(html_to_parse, 'lxml') # lxml íŒŒì„œ ì‚¬ìš©

        # ì±… ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” <article class="product_pod"> íƒœê·¸ë“¤ì„ ëª¨ë‘ ì°¾ìŒ
        book_articles = soup.find_all('article', class_='product_pod')
        
        if book_articles:
            st.success(f"ì´ {len(book_articles)}ê°œì˜ ì±… ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ìƒìœ„ 5ê°œë§Œ í‘œì‹œ)")
            scraped_data = []
            for i, article in enumerate(book_articles[:5]): # ìƒìœ„ 5ê°œë§Œ ì²˜ë¦¬
                # ì±… ì œëª©: <h3> íƒœê·¸ ì•ˆì˜ <a> íƒœê·¸ì˜ title ì†ì„± ë˜ëŠ” í…ìŠ¤íŠ¸
                title_tag = article.find('h3').find('a')
                title = title_tag['title'] if title_tag and 'title' in title_tag.attrs else title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"
                
                # ì±… ê°€ê²©: <p class="price_color"> íƒœê·¸ì˜ í…ìŠ¤íŠ¸
                price_tag = article.find('p', class_='price_color')
                price = price_tag.get_text(strip=True) if price_tag else "ê°€ê²© ì •ë³´ ì—†ìŒ"
                
                # (ì„ íƒ) ì±… ë§í¬: <h3> íƒœê·¸ ì•ˆì˜ <a> íƒœê·¸ì˜ href ì†ì„±
                link = title_tag['href'] if title_tag and 'href' in title_tag.attrs else "ë§í¬ ì—†ìŒ"
                full_link = requests.compat.urljoin(scrape_target_url, link) # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                
                scraped_data.append({'Title': title, 'Price': price, 'Link': full_link})
            
            df_scraped = pd.DataFrame(scraped_data)
            st.dataframe(df_scraped)

            # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš© ì˜ˆì‹œ (ì œëª©ë§Œ ì¶”ì¶œ)
            # titles_elements = [article.find('h3').find('a') for article in book_articles if article.find('h3') and article.find('h3').find('a')]
            # display_scraped_elements(titles_elements, title="ì¶”ì¶œëœ ì±… ì œëª© (ì¼ë¶€)", element_description="ì œëª©", max_elements_to_show=5)

        else:
            st.warning("ì±… ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” `article.product_pod` ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    except ImportError:
        st.error("`lxml` íŒŒì„œê°€ í•„ìš”í•©ë‹ˆë‹¤. `pip install lxml` ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    except requests.exceptions.RequestException as e:
        st.error(f"ì›¹ í˜ì´ì§€ ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    except Exception as e:
        st.error(f"HTML íŒŒì‹± ë˜ëŠ” ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


st.markdown("---")
st.subheader("4.3 ì›¹ ìŠ¤í¬ë ˆì´í•‘ì˜ í•œê³„ì™€ ì£¼ì˜ì‚¬í•­ (ì¬ê°•ì¡°)")
st.markdown("""
- **ì •ì  vs. ë™ì  ì›¹ì‚¬ì´íŠ¸:** `requests`ì™€ `BeautifulSoup`ì€ ì£¼ë¡œ ì •ì ì¸ HTML ë‚´ìš©ì„ ê°€ì ¸ì™€ íŒŒì‹±í•©ë‹ˆë‹¤. JavaScriptë¥¼ í†µí•´ ë™ì ìœ¼ë¡œ ë‚´ìš©ì´ ë¡œë“œë˜ê±°ë‚˜ ë³€ê²½ë˜ëŠ” ì›¹ì‚¬ì´íŠ¸(Single Page Applications ë“±)ì˜ ë°ì´í„°ëŠ” ì´ ë°©ë²•ë§Œìœ¼ë¡œëŠ” ìˆ˜ì§‘í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì´ ê²½ìš° Selenium, Playwrightì™€ ê°™ì€ ë¸Œë¼ìš°ì € ìë™í™” ë„êµ¬ê°€ í•„ìš”í•˜ë©°, ì´ëŠ” ë” ë³µì¡í•œ ê¸°ìˆ ì…ë‹ˆë‹¤.
- **ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½:** ì›¹ì‚¬ì´íŠ¸ì˜ HTML êµ¬ì¡°ëŠ” ì–¸ì œë“ ì§€ ë³€ê²½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. êµ¬ì¡°ê°€ ë³€ê²½ë˜ë©´ ê¸°ì¡´ ìŠ¤í¬ë ˆì´í•‘ ì½”ë“œê°€ ë” ì´ìƒ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì½”ë“œì˜ ìœ ì§€ë³´ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.
- **CAPTCHA ë° ì•ˆí‹°-ìŠ¤í¬ë ˆì´í•‘ ê¸°ìˆ :** ë§ì€ ì›¹ì‚¬ì´íŠ¸ëŠ” CAPTCHA, IP ê¸°ë°˜ ì°¨ë‹¨, ì‚¬ìš©ì ì—ì´ì „íŠ¸ ê²€ì‚¬ ë“± ë‹¤ì–‘í•œ ì•ˆí‹°-ìŠ¤í¬ë ˆì´í•‘ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ìë™í™”ëœ ì ‘ê·¼ì„ ë§‰ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë³´í˜¸ ì¥ì¹˜ë¥¼ ìš°íšŒí•˜ë ¤ëŠ” ì‹œë„ëŠ” ì„œë¹„ìŠ¤ ì•½ê´€ ìœ„ë°˜ì´ë©° í”¼í•´ì•¼ í•©ë‹ˆë‹¤.
- **ë²•ì /ìœ¤ë¦¬ì  ì±…ì„:** ì•ì„œ ê°•ì¡°í–ˆë“¯ì´, í•­ìƒ í•©ë²•ì ì´ê³  ìœ¤ë¦¬ì ì¸ ë²”ìœ„ ë‚´ì—ì„œë§Œ ìŠ¤í¬ë ˆì´í•‘ì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.

**í•™ìŠµ ëª©ì ìœ¼ë¡œ ìŠ¤í¬ë ˆì´í•‘ì„ ì—°ìŠµí•  ë•ŒëŠ” `books.toscrape.com`ì´ë‚˜ `toscrape.com`ê³¼ ê°™ì´ ìŠ¤í¬ë ˆì´í•‘ì„ ìœ„í•´ ëª…ì‹œì ìœ¼ë¡œ ì œê³µëœ ì›¹ì‚¬ì´íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤.**
""")